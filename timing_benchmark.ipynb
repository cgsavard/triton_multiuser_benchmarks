{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252e6869",
   "metadata": {},
   "source": [
    "# Timing benchmark\n",
    "\n",
    "This notebook will compare the timing difference between using a local torch model vs. a torch model hosted on the triton server. The model we are working with is an example of the ParticleNet model seen [here](https://cms-ml.github.io/documentation/inference/particlenet.html).\n",
    "\n",
    "To test your own model, you will need to first prepare the model for the inference server. An example of the model conversion can be found in the ```model_conversion.ipynb``` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b05b8",
   "metadata": {},
   "source": [
    "Let's load in the 2 different types of taggers we will look at (changes made here to switch out models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17350c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13543ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ParticleNet import ParticleNetTagger\n",
    "import torch\n",
    "\n",
    "# load in local model\n",
    "local_model = ParticleNetTagger(5, 2,\n",
    "                        [(16, (64, 64, 64)), (16, (128, 128, 128)), (16, (256, 256, 256))],\n",
    "                        [(256, 0.1)],\n",
    "                        use_fusion=True,\n",
    "                        use_fts_bn=False,\n",
    "                        use_counts=True,\n",
    "                        for_inference=False)\n",
    "\n",
    "LOCAL_PATH = \"pn_demo.pt\"\n",
    "local_model.load_state_dict(torch.load(LOCAL_PATH, map_location=torch.device('cpu')))\n",
    "local_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f70be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tritonutils import wrapped_triton\n",
    "\n",
    "# create instance of triton model\n",
    "triton_model = wrapped_triton( \"triton+grpc://triton.apps.okddev.fnal.gov:443/pn_demo/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71f9aa",
   "metadata": {},
   "source": [
    "We will double check that the outputs of the local and triton models match within 10^-5 before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 random jets with 100 tracks each\n",
    "test_inputs = {'points': np.random.rand(5,2,100).astype(np.float32),\n",
    "               'features': np.random.rand(5,5,100).astype(np.float32),\n",
    "               'mask': np.ones((5,1,100),dtype=np.float32)}\n",
    "\n",
    "# slighlty different inputs for each model\n",
    "test_inputs_local = []\n",
    "test_inputs_triton = {}\n",
    "c = 0\n",
    "for k in test_inputs.keys():\n",
    "    test_inputs_local.append(torch.from_numpy(test_inputs[k]))\n",
    "    test_inputs_triton[f'{k}__{c}'] = test_inputs[k]\n",
    "    c += 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    local_output = local_model(*test_inputs_local).detach().numpy()\n",
    "triton_output = triton_model(test_inputs_triton)\n",
    "np.testing.assert_almost_equal(local_output, triton_output, decimal=5, err_msg='Outputs do NOT match')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea484d",
   "metadata": {},
   "source": [
    "Next, let's create a much large sample of data to test the timing between the different model versions. We will use the [awkward array](https://awkward-array.org/doc/main/) structure to hold the inputs because it is easier to adapt to both the local and triton model when batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dab22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 10000 random jets with 100 tracks each\n",
    "test_inputs = {'points': np.random.rand(10000,2,100).astype(np.float32),\n",
    "               'features': np.random.rand(10000,5,100).astype(np.float32),\n",
    "               'mask': np.ones((10000,1,100),dtype=np.float32)}\n",
    "\n",
    "test_inputs_ak = ak.Array(test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af4426",
   "metadata": {},
   "source": [
    "The inputs (jets in the ParticleNet case) are batched as they are processed. The batch size should be determined based on what is most efficient for the current model being used. For this demo model, a batch size of 1024 is used but this variable can be changed if desired.\n",
    "\n",
    "To test the timing differences between the two models, we will collect the time that has passed for each new batch of data and compare. Here is a function that will take in the full dataset, then batch and run either the local or triton model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jets(in_jets, batch_size=1024, use_triton=False):\n",
    "    \n",
    "    print('Running triton server inference' if use_triton else 'Running local inference')\n",
    "    \n",
    "    # define variables to track processing time\n",
    "    njets = []\n",
    "    t = []\n",
    "    t_begin = time.time()\n",
    "    \n",
    "    # loop through input data batches and run inference on each batch\n",
    "    for ii in range(0, len(in_jets), batch_size):\n",
    "        print('%i/%i jets processed, processing next batch'%(ii,len(in_jets)))\n",
    "\n",
    "        # get a batch of data\n",
    "        try:\n",
    "            jets_eval = in_jets[ii:ii + batch_size]\n",
    "            njets.append(ii+batch_size)\n",
    "        except:\n",
    "            jets_eval = in_jets[ii:-1]\n",
    "            njets.append(len(in_jets))\n",
    "\n",
    "        ## structure inputs slightly differently and run inference depending on model\n",
    "        # triton model\n",
    "        if use_triton:\n",
    "            X = {}\n",
    "            c = 0\n",
    "            for k in jets_eval.fields:\n",
    "                X[f'{k}__{c}'] = ak.to_numpy(jets_eval[k])\n",
    "                c += 1\n",
    "                \n",
    "            # triton inference\n",
    "            outputs = triton_model(X)\n",
    "                \n",
    "        # local model   \n",
    "        else:\n",
    "            X = []\n",
    "            for k in jets_eval.fields:\n",
    "                X.append(torch.from_numpy(ak.to_numpy(jets_eval[k])))\n",
    "                \n",
    "            # local inference\n",
    "            with torch.no_grad():\n",
    "                outputs = local_model(*X).detach().numpy()\n",
    "\n",
    "        t.append(time.time()-t_begin)\n",
    "        \n",
    "    print('Total time elapsed = %f sec'%t[-1])\n",
    "\n",
    "    return njets, t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4970007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_njets, local_t = process_jets(test_inputs_ak, use_triton=False, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_njets, triton_t = process_jets(test_inputs_ak, use_triton=True, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce0157",
   "metadata": {},
   "source": [
    "Now we can plot some of the results and compare then between the two inference methods. We will use matplotlib as our plotting tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8352f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.CMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0f2e1",
   "metadata": {},
   "source": [
    "Next, let's take a look at the time you gain when using triton as opposed to a local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "# set height ratios for subplots\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])\n",
    "\n",
    "# the first subplot\n",
    "ax0 = plt.subplot(gs[0])\n",
    "# log scale for axis Y of the first subplot\n",
    "ax0.set_yscale(\"log\")\n",
    "line0, = ax0.plot(local_njets, local_t, color='r')\n",
    "line1, = ax0.plot(triton_njets, triton_t, color='b')\n",
    "\n",
    "# the second subplot\n",
    "# shared axis X\n",
    "ax1 = plt.subplot(gs[1], sharex = ax0)\n",
    "line2, = ax1.plot(local_njets, np.array(local_t)/np.array(triton_t), color='black', linestyle='--')\n",
    "plt.setp(ax0.get_xticklabels(), visible=False)\n",
    "# remove last tick label for the second subplot\n",
    "yticks = ax1.yaxis.get_major_ticks()\n",
    "yticks[-1].label1.set_visible(False)\n",
    "\n",
    "# put legend on first subplot\n",
    "ax0.legend((line0, line1), ('local model', 'triton model'), loc='lower left')\n",
    "\n",
    "ax0.set_ylabel('time elapsed (s)')\n",
    "ax1.set_ylabel('$t_{local}/t_{triton}$')\n",
    "ax1.set_xlabel('# jets processed')\n",
    "\n",
    "# remove vertical gap between subplots\n",
    "plt.subplots_adjust(hspace=.0)\n",
    "plt.rcParams[\"figure.figsize\"] = (7,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba7c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
