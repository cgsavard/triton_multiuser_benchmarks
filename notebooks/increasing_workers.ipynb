{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48794d39",
   "metadata": {},
   "source": [
    "# Increasing workers benchmark\n",
    "\n",
    "In this notebook, we will study the performance of the triton server as more inference requests are made to the server in parallel. We will begin by creating a cluster of workers that will each have a copy of code that runs inference. We will then ask 1, then 2, then 3, and so on, workers to run the code consecutively and collect a couple metrics, such as throughput and queue time, to see how the triton server handles the increase in requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688491cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the packages we will need\n",
    "import os\n",
    "from distributed import Client, progress\n",
    "from lpcjobqueue import LPCCondorCluster\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.mlbench import process_function\n",
    "import time\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "from utils.promqueries import get_all_queries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34622120",
   "metadata": {},
   "source": [
    "This will request a cluster with $n$ workers (or jobs) of CPUs from the Fermilab LPC. If not using the Fermilab computing centers, this will need to be changed for the system you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e375707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(jobs, **kwargs):\n",
    "    \n",
    "    kwargs.setdefault('cores', 1)\n",
    "    kwargs.setdefault('memory', '3GB')\n",
    "    kwargs.setdefault('disk', '2GB')\n",
    "    # by default transfer all utils and models\n",
    "    kwargs.setdefault('transfer_input_files', [f'{os.getenv(\"BASE\")}/utils', f'{os.getenv(\"BASE\")}/models'])\n",
    "    kwargs.setdefault('log_directory', None)\n",
    "    kwargs.setdefault('death_timeout', 180)\n",
    "    kwargs.setdefault('job_extra_directives', {})\n",
    "    kwargs['job_extra_directives'].update(set_default_proxy(kwargs['job_extra_directives']))\n",
    "\n",
    "    cluster = LPCCondorCluster(**kwargs)\n",
    "\n",
    "    # Scaling up the cluster\n",
    "    print(\"Generating job requests...\", end='')\n",
    "    cluster.scale(jobs)\n",
    "    print('initial jobs generated!')\n",
    "    print(\"Waiting for at least one worker...\", end='')\n",
    "    client = Client(cluster)\n",
    "    client.wait_for_workers(1)\n",
    "    print(\"workers(s) online!\")\n",
    "    print(\"Dashboard available at\", client.dashboard_link)\n",
    "    print(\"Waiting for all (%i) workers...\"%jobs, end='')\n",
    "    client.wait_for_workers(jobs)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return cluster, client\n",
    "\n",
    "def set_default_proxy(job_extra_directives):\n",
    "  \"\"\"\n",
    "  Specifying the the grid certificate proxy to be used by the worker nodes. As\n",
    "  the voms-proxy-init typically store certificates the `/tmp` directory, which is\n",
    "  not accessible to the worker nodes. The returned job_extra_directives will\n",
    "  setup the worker nodes to look for the proxy file in the users home directory.\n",
    "  This function will also scan the input proxy file to make sure it exists and is\n",
    "  valid. If the file is not found, an exception is raised with the command to\n",
    "  generate the proxy file in default location.\n",
    "  \"\"\"\n",
    "  proxyfile = ''\n",
    "  if 'x509userproxy' not in job_extra_directives:\n",
    "    proxyfile = '{0}/x509up_u{1}'.format(os.environ['HOME'], os.getuid())\n",
    "    print('Using default proxy file:', proxyfile)\n",
    "  else:\n",
    "    proxyfile = job_extra_directives['x509userproxy']\n",
    "\n",
    "  # Checking if file is a valid file\n",
    "  if not os.path.isfile(proxyfile):\n",
    "    raise Exception(f\"\"\"\n",
    "    The proxy file {proxyfile} doesn't exist! Create the default proxy using the\n",
    "    following command:\n",
    "    > voms-proxy-init --voms cms --valid 192:00 --out ${{HOME}}/x509up_u${{UID}}\n",
    "    \"\"\")\n",
    "\n",
    "  return {'x509userproxy': proxyfile}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0936ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# include a path for outputting log files for monitoring and debugging purposes\n",
    "cluster_args = {'log_directory': '/uscmst1b_scratch/lpc1/3DayLifetime/<username>/'}\n",
    "n_workers = 80\n",
    "cluster, client = create_clusters(n_workers, **cluster_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff3f3b",
   "metadata": {},
   "source": [
    "Here, we will start $n$ trails with different number of workers. Each trial will split files between the workers, process all files consecutively, then save the times of each trial so that we can go back and look at what happened after the run is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c115636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "worker_hostnames = list(client.scheduler_info()['workers'].keys())\n",
    "output = np.zeros((n_workers,3))\n",
    "datetimes = []\n",
    "\n",
    "for ii in range(n_workers):\n",
    "\n",
    "    #seeds, #pseudo-events, batchsize, use triton (True/False)\n",
    "    n_files = 30*(ii+1) # run 30 files per worker\n",
    "    n_jets = 5000 # run 5000 jets per file\n",
    "    model = \"pn_demo/1\"\n",
    "    workargstriton = [range(n_files), [n_jets]*n_files, [1024]*n_files, \n",
    "                      [True]*n_files, [model]*n_files]\n",
    "\n",
    "    # Triton, N workers trial\n",
    "    print('Running %i jets among %i files with %i workers...'%(n_jets,n_files,ii+1))\n",
    "    dt1 = datetime.now()\n",
    "    futurestriton = client.map(process_function, *workargstriton, pure=False, \n",
    "                               workers=worker_hostnames[:ii+1], retries=2)\n",
    "    progress(futurestriton, notebook=False) #progress bar\n",
    "    resulttriton = client.gather(futurestriton)\n",
    "    dt2 = datetime.now()\n",
    "    print('Done!')\n",
    "    \n",
    "    datetimes.append((dt1,dt2,ii+1))\n",
    "    \n",
    "# save the datetimes of each trial to file to look at later\n",
    "with open('datetimes_saved.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s, %s, %s' % x for x in datetimes))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to close all jobs when the trials are done\n",
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56664abe",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Now that we have run the tests, let's take a look at the Triton server performance. We start by loading in the datetimes of the test we ran. And using those times to collect the metrics we would like to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6108b69d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open saved data and put back into proper format\n",
    "#with open('datetimes_saved.txt') as f:\n",
    "with open('datetimes_Apr30_80workers_v1.txt') as f:\n",
    "    mylist = [tuple(map(str.strip, i.split(','))) for i in f]\n",
    "    \n",
    "datetimes = []\n",
    "for tup in mylist:\n",
    "    datetimes.append((datetime.strptime(tup[0], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                       datetime.strptime(tup[1], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                       int(tup[2])))\n",
    "datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abfd1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collect metrics from each trial to plots performance\n",
    "out = np.zeros((len(datetimes),3))\n",
    "out_cat = np.empty((0,3), int)\n",
    "for (dt1,dt2,w) in datetimes:\n",
    "    results, queries, unique_model_versions, unique_gpu_instances = get_all_queries([(dt1,dt2)], '30s')\n",
    "    \n",
    "    # metrics collected can be changed here\n",
    "    data = pd.concat([results['inf_reqs_net'],results['inf_que_time_net'],results['num_instances']],axis=1)\n",
    "    n_inst = data.iloc[:,2].max()\n",
    "    out[w-1,0] = data.iloc[:,0][data.iloc[:,2]==n_inst][1:-1].mean()\n",
    "    out[w-1,1] = data.iloc[:,1][data.iloc[:,2]==n_inst][1:-1].mean()\n",
    "    out[w-1,2] = n_inst\n",
    "    \n",
    "    # all metrics aggregated for each trial\n",
    "    out_cat = np.append(out_cat,data.to_numpy(),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de289531",
   "metadata": {},
   "source": [
    "There are 4 plots provided here:\n",
    "\n",
    "1. Number of triton instances vs. number of workers - this shows how the triton server scales out to more model instances as more requests are made in parallel\n",
    "2. Queue time vs. total throughput - this shows how another model instance is spawned up once the queue threshold is surpassed in order to maintain a reasonable queue time (as GPU resources are available to create another instance)\n",
    "3. Total throughput vs. number of workers - this shows whether the throughput is scaling pretty linearly with the number of workers or not\n",
    "4. Throughput per model instance vs. number of workers - this shows how multiple parallel inferences affect the throughput of each model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(out[:,2])+1),out[:,2])\n",
    "plt.xlabel(\"Number of workers\", fontsize=14)\n",
    "plt.ylabel(\"Number of Triton instances\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.plasma\n",
    "norm = colors.BoundaryNorm(np.arange(min(out[:,2])-.5, max(out[:,2])+1.5, 1), cmap.N)\n",
    "plt.scatter(out[:,0],out[:,1],c=out[:,2], cmap=cmap, norm=norm)\n",
    "cbar = plt.colorbar(ticks=np.arange(min(out[:,2]), max(out[:,2])+1, 1))\n",
    "cbar.set_label('Triton server instances', fontsize=14)\n",
    "plt.xlabel('Total throughput [$s^{-1}$]',fontsize=14)\n",
    "plt.ylabel('Queue time per request [$ms$]',fontsize=14)\n",
    "plt.axhline(y=400, color='r', linestyle='--', label='Queue time threshold')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.plasma\n",
    "norm = colors.BoundaryNorm(np.arange(min(out[:,2])-.5, max(out[:,2])+1.5, 1), cmap.N)\n",
    "plt.scatter(range(1,len(out[:,0])+1),out[:,0],c=out[:,2], cmap=cmap, norm=norm)\n",
    "cbar = plt.colorbar(ticks=np.arange(min(out[:,2]), max(out[:,2])+1, 1))\n",
    "cbar.set_label('Triton server instances', fontsize=14)\n",
    "plt.xlabel('Number of workers', fontsize=14)\n",
    "plt.ylabel('Total throughput [$s^{-1}$]', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.plasma\n",
    "norm = colors.BoundaryNorm(np.arange(min(out[:,2])-.5, max(out[:,2])+1.5, 1), cmap.N)\n",
    "plt.scatter(range(1,len(out[:,0])+1)/out[:,2],out[:,0]/out[:,2],c=out[:,2], cmap=cmap, norm=norm)\n",
    "cbar = plt.colorbar(ticks=np.arange(min(out[:,2]), max(out[:,2])+1, 1))\n",
    "cbar.set_label('Triton server instances', fontsize=14)\n",
    "plt.xlabel('Number of workers per server', fontsize=14)\n",
    "plt.ylabel('Throughput per server [$s^{-1}$]', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e402db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea for triton",
   "language": "python",
   "name": "coffea-triton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
