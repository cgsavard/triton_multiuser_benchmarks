{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tritonutils import wrapped_triton\n",
    "\n",
    "# create instance of triton model\n",
    "triton_model = wrapped_triton( \"triton+grpc://test-3.apps.okddev.fnal.gov:443/reconstruction_bdt_xgb/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local model\n",
    "from xgboost import XGBClassifier\n",
    "local_model = XGBClassifier()\n",
    "local_model.load_model('/srv/models/xgb_demo.json')\n",
    "local_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small test\n",
    "data = np.random.rand(5,20).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12defe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing local model\n",
    "local_model.predict_proba(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aaa613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing triton model\n",
    "out = triton_model({'input__0':data},'output__0')\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5682a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jets(in_jets, batch_size=10000, use_triton=False):\n",
    "    \n",
    "    print('Running triton server inference' if use_triton else 'Running local inference')\n",
    "    \n",
    "    # define variables to track processing time\n",
    "    njets = np.array([])\n",
    "    t = np.array([])\n",
    "    t_begin = time.time()\n",
    "    \n",
    "    # loop through input data batches and run inference on each batch\n",
    "    for ii in range(0, len(in_jets), batch_size):\n",
    "        print('%i/%i jets processed, processing next batch'%(ii,len(in_jets)))\n",
    "\n",
    "        # get a batch of data\n",
    "        try:\n",
    "            jets_eval = in_jets[ii:ii + batch_size]\n",
    "            njets = np.append(njets, ii+batch_size)\n",
    "        except:\n",
    "            jets_eval = in_jets[ii:-1]\n",
    "            njets = np.append(njets, len(in_jets))\n",
    "\n",
    "        ## structure inputs slightly differently and run inference depending on model\n",
    "        # triton model\n",
    "        if use_triton:\n",
    "            X = {}\n",
    "            c = 0\n",
    "            for k in jets_eval.fields:\n",
    "                X[f'{k}__{c}'] = ak.to_numpy(jets_eval[k])\n",
    "                c += 1\n",
    "                \n",
    "            # triton inference\n",
    "            outputs = triton_model(X)\n",
    "                \n",
    "        # local model   \n",
    "        else:\n",
    "            X = []\n",
    "            for k in jets_eval.fields:\n",
    "                X.append(torch.from_numpy(ak.to_numpy(jets_eval[k])))\n",
    "                \n",
    "            # local inference\n",
    "            with torch.no_grad():\n",
    "                outputs = local_model(*X).detach().numpy()\n",
    "\n",
    "        t = np.append(t, time.time()-t_begin)\n",
    "        \n",
    "    print('Total time elapsed = %f sec'%t[-1])\n",
    "\n",
    "    return njets, t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf845394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random inputs to test\n",
    "n_inputs = 10000000\n",
    "test_inputs = {'input': np.random.rand(n_inputs,20).astype(np.float32)}\n",
    "\n",
    "test_inputs_ak = ak.Array(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e451ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jets(in_jets, batch_size=1024, use_triton=False):\n",
    "    \n",
    "    print('Running triton server inference' if use_triton else 'Running local inference')\n",
    "    \n",
    "    # define variables to track processing time\n",
    "    njets = np.array([])\n",
    "    t = np.array([])\n",
    "    t_begin = time.time()\n",
    "    \n",
    "    # loop through input data batches and run inference on each batch\n",
    "    for ii in range(0, len(in_jets), batch_size):\n",
    "        print('%i/%i jets processed, processing next batch'%(ii,len(in_jets)))\n",
    "\n",
    "        # get a batch of data\n",
    "        try:\n",
    "            jets_eval = in_jets[ii:ii + batch_size]\n",
    "            njets = np.append(njets, ii+batch_size)\n",
    "        except:\n",
    "            jets_eval = in_jets[ii:-1]\n",
    "            njets = np.append(njets, len(in_jets))\n",
    "\n",
    "        ## structure inputs slightly differently and run inference depending on model\n",
    "        # triton model\n",
    "        if use_triton:\n",
    "            X = {}\n",
    "            c = 0\n",
    "            for k in jets_eval.fields:\n",
    "                X[f'{k}__{c}'] = ak.to_numpy(jets_eval[k])\n",
    "                c += 1\n",
    "                \n",
    "            # triton inference\n",
    "            outputs = triton_model(X)\n",
    "                \n",
    "        # local model   \n",
    "        else:\n",
    "            for k in jets_eval.fields:\n",
    "                X = ak.to_numpy(jets_eval[k])\n",
    "                \n",
    "            # local inference\n",
    "            outputs = local_model.predict_proba(X)\n",
    "\n",
    "        t = np.append(t, time.time()-t_begin)\n",
    "        \n",
    "    print('Total time elapsed = %f sec'%t[-1])\n",
    "\n",
    "    return njets, t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4075b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to minimize noise due to connection with triton server, take avg results of n trials\n",
    "n = 1\n",
    "local_t = None\n",
    "for ii in range(n):\n",
    "    print('------ Trial %i ------'%ii)\n",
    "    if local_t is None:\n",
    "        local_njets, local_t = process_jets(test_inputs_ak, use_triton=False, batch_size=100000)\n",
    "    else:\n",
    "        local_njets, local_t_temp = process_jets(test_inputs_ak, use_triton=False, batch_size=100000)\n",
    "        local_t += local_t_temp\n",
    "local_t /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8121b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to minimize noise due to connection with triton server, take avg results of n trials\n",
    "n = 10\n",
    "triton_t = None\n",
    "for ii in range(n):\n",
    "    print('------ Trial %i ------'%ii)\n",
    "    if triton_t is None:\n",
    "        triton_njets, triton_t = process_jets(test_inputs_ak, use_triton=True, batch_size=100000)\n",
    "    else:\n",
    "        triton_njets, triton_t_temp = process_jets(test_inputs_ak, use_triton=True, batch_size=100000)\n",
    "        triton_t += triton_t_temp\n",
    "triton_t /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING WITHOUT ANY EXTRA FLUFF, just choose the batch size and time to test for\n",
    "c = 0\n",
    "batch_size = 1000000\n",
    "print(datetime.now())\n",
    "t_start = time.time()\n",
    "while (time.time()-t_start)<480: # 60*minutes you want to run\n",
    "    data = np.random.rand(batch_size,20).astype(np.float32)\n",
    "    out = triton_model({'input__0':data},'output__0')\n",
    "    c +=1\n",
    "print(datetime.now())\n",
    "print('%i batches processed, batch size of %i'%(c,batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb98f7",
   "metadata": {},
   "source": [
    "## now plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.CMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "# set height ratios for subplots\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])\n",
    "\n",
    "# the first subplot\n",
    "ax0 = plt.subplot(gs[0])\n",
    "# log scale for axis Y of the first subplot\n",
    "ax0.set_yscale(\"log\")\n",
    "line0, = ax0.plot(local_njets, local_t, color='r')\n",
    "line1, = ax0.plot(triton_njets, triton_t, color='b')\n",
    "\n",
    "# the second subplot\n",
    "# shared axis X\n",
    "ax1 = plt.subplot(gs[1], sharex = ax0)\n",
    "line2, = ax1.plot(local_njets, local_t/triton_t, color='black', linestyle='--')\n",
    "plt.setp(ax0.get_xticklabels(), visible=False)\n",
    "# remove last tick label for the second subplot\n",
    "yticks = ax1.yaxis.get_major_ticks()\n",
    "yticks[-1].label1.set_visible(False)\n",
    "\n",
    "# put legend on first subplot\n",
    "ax0.legend((line0, line1), ('local model', 'triton model'), loc='lower right')\n",
    "\n",
    "ax0.set_ylabel('time elapsed (s)')\n",
    "ax1.set_ylabel('$t_{local}/t_{triton}$')\n",
    "ax1.set_xlabel('# inputs processed')\n",
    "\n",
    "# remove vertical gap between subplots\n",
    "plt.subplots_adjust(hspace=.0)\n",
    "plt.rcParams[\"figure.figsize\"] = (7,6)\n",
    "#plt.savefig(\"results/timing_results_xgb.eps\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d723435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
