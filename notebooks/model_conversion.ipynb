{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272b73a9",
   "metadata": {},
   "source": [
    "# Convert model for triton server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2d60f",
   "metadata": {},
   "source": [
    "This notebook will show an example of how to convert a model so that it can be used by the triton server.\n",
    "\n",
    "Start by loading in the model you want to use. This will be called the local model because it is stored locally on the LPC nodes and will be used locally. Here, we are using a ParticleNet model which and uses PyTorch as a backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc50e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models.ParticleNet import ParticleNetTagger\n",
    "import torch\n",
    "\n",
    "# load in local model\n",
    "local_model = ParticleNetTagger(5, 2,\n",
    "                        [(16, (64, 64, 64)), (16, (128, 128, 128)), (16, (256, 256, 256))],\n",
    "                        [(256, 0.1)],\n",
    "                        use_fusion=True,\n",
    "                        use_fts_bn=False,\n",
    "                        use_counts=True,\n",
    "                        for_inference=False)\n",
    "\n",
    "LOCAL_PATH = \"/srv/models/pn_demo.pt\"\n",
    "local_model.load_state_dict(torch.load(LOCAL_PATH, map_location=torch.device('cpu')))\n",
    "local_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf71e069",
   "metadata": {},
   "source": [
    "To upload the model to the triton server, it must be converted with jit and then some configuration files must be created. Examples of the structure of the triton files can be found [here](https://github.com/fastmachinelearning/sonic-models/tree/master/models/particlenet) and more on the specifics of configuration can be found [here](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f283c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert model with jit and save to file\n",
    "jit_model = torch.jit.script(local_model)\n",
    "JIT_PATH = '/srv/models/jit_model_demo.pt'\n",
    "torch.jit.save(jit_model, JIT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8ac53",
   "metadata": {},
   "source": [
    "Examples of the configuration files needed by the triton server:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd7a08b7",
   "metadata": {},
   "source": [
    "# config.pbtxt\n",
    "\n",
    "name: \"pn_demo\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size : 1200\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: [ 1024 ]\n",
    "}\n",
    "input [\n",
    "  {\n",
    "    name: \"points__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 2, -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"features__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 5, -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"mask__2\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1, -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"softmax__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 2 ]\n",
    "    label_filename: \"pn_demo_labels.txt\"\n",
    "  }\n",
    "]\n",
    "parameters: {\n",
    "key: \"INFERENCE_MODE\"\n",
    "    value: {\n",
    "    string_value: \"true\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc1cfeda",
   "metadata": {},
   "source": [
    "# pn_demo_labels.txt\n",
    "\n",
    "is_true\n",
    "is_false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e1fca",
   "metadata": {},
   "source": [
    "Now the jitted torch model can be uploaded to the triton server at the multi-user facility. This demo hosts the server at [EAF](https://indico.cern.ch/event/903719/contributions/3803524/attachments/2013546/3364991/Elastic_AF_-_Fermilab.pdf) but the server path \"triton+grpc://triton.apps.okddev.fnal.gov:443/MODEL_NAME/1\" will need to be changed to the proper path used by your triton server.\n",
    "\n",
    "Next, we will create a client that connects to the triton server. This will be created as a class to use the triton model in the same way as the local model. This class is modeled off of the code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c2595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as triton_grpc\n",
    "import numpy as np\n",
    "\n",
    "class wrapped_triton:\n",
    "  def __init__(self, model_url: str, ) -> None:\n",
    "    fullprotocol, location = model_url.split(\"://\")\n",
    "    _, protocol = fullprotocol.split(\"+\")\n",
    "    address, model, version = location.split(\"/\")\n",
    "\n",
    "    self._protocol = protocol\n",
    "    self._address = address\n",
    "    self._model = model\n",
    "    self._version = version\n",
    "\n",
    "    # check connection to server, throw error if connection doesn't work\n",
    "    if self._protocol == \"grpc\":\n",
    "      self._client = triton_grpc.InferenceServerClient(url=self._address,\n",
    "                                                       verbose=False,\n",
    "                                                       ssl=True)\n",
    "      self._triton_protocol = triton_grpc\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          f\"{self._protocol} does not encode a valid protocol (grpc or http)\")\n",
    "\n",
    "  def __call__(self, input_dict) -> np.ndarray:\n",
    "    '''\n",
    "    Run inference of model on triton server\n",
    "    '''\n",
    "\n",
    "    # put inputs in proper format\n",
    "    inputs = []\n",
    "    for key in input_dict:\n",
    "      input = self._triton_protocol.InferInput(key, input_dict[key].shape,\n",
    "                                               \"FP32\")\n",
    "      input.set_data_from_numpy(input_dict[key])\n",
    "      inputs.append(input)\n",
    "\n",
    "    output = self._triton_protocol.InferRequestedOutput(\"softmax__0\")\n",
    "\n",
    "    # make request to server for inference\n",
    "    request = self._client.infer(self._model,\n",
    "                                 model_version=self._version,\n",
    "                                 inputs=inputs,\n",
    "                                 outputs=[output],\n",
    "                                 )\n",
    "    out = request.as_numpy(\"softmax__0\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9670f3",
   "metadata": {},
   "source": [
    "Now we will create an instance of the triton version of the ParticleNet model. This instance will point towards the triton server hosted on EAF and inference will be called in the same way as with the local torch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ac8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_model = wrapped_triton( \"triton+grpc://triton.fnal.gov:443/pn_demo/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613aa558",
   "metadata": {},
   "source": [
    "We can now test both the local and triton models and see if they return the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de898bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 random jets with 100 tracks each\n",
    "test_inputs = {'points': np.random.rand(5,2,100).astype(np.float32),\n",
    "               'features': np.random.rand(5,5,100).astype(np.float32),\n",
    "               'mask': np.ones((5,1,100),dtype=np.float32)}\n",
    "\n",
    "# slighlty different inputs for each model\n",
    "test_inputs_local = []\n",
    "test_inputs_triton = {}\n",
    "c = 0\n",
    "for k in test_inputs.keys():\n",
    "    test_inputs_local.append(torch.from_numpy(test_inputs[k]))\n",
    "    test_inputs_triton[f'{k}__{c}'] = test_inputs[k]\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model(*test_inputs_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_model(test_inputs_triton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35dea7",
   "metadata": {},
   "source": [
    "The results match! Woohoo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd30db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
