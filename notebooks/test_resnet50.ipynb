{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the local model\n",
    "import torch\n",
    "\n",
    "torch.hub._validate_not_a_forked_repo = lambda a, b, c: True\n",
    "local_model = torch.hub.load(\"pytorch/vision:v0.10.0\", \"resnet50\", pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tritonutils import wrapped_triton\n",
    "\n",
    "# create instance of triton model\n",
    "triton_model = wrapped_triton( \"triton+grpc://test-3.apps.okddev.fnal.gov:443/resnet50_torch/1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random data\n",
    "data = np.random.rand(10,3,224,224).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12defe41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test local\n",
    "with torch.no_grad():\n",
    "    local_output = local_model(torch.from_numpy(data))#.detach().numpy()\n",
    "print(local_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aaa613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test triton\n",
    "out = triton_model({'input__0':data},'output__0')\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf845394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random inputs to test\n",
    "n_inputs = 2000\n",
    "test_inputs = {'input': np.random.rand(n_intputs,3,224,224).astype(np.float32)}\n",
    "test_inputs_ak = ak.Array(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e451ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jets(in_jets, batch_size=1024, use_triton=False):\n",
    "    \n",
    "    print('Running triton server inference' if use_triton else 'Running local inference')\n",
    "    \n",
    "    # define variables to track processing time\n",
    "    njets = np.array([])\n",
    "    t = np.array([])\n",
    "    t_begin = time.time()\n",
    "    \n",
    "    # loop through input data batches and run inference on each batch\n",
    "    for ii in range(0, len(in_jets), batch_size):\n",
    "        print('%i/%i jets processed, processing next batch'%(ii,len(in_jets)))\n",
    "\n",
    "        # get a batch of data\n",
    "        try:\n",
    "            jets_eval = in_jets[ii:ii + batch_size]\n",
    "            njets = np.append(njets, ii+batch_size)\n",
    "        except:\n",
    "            jets_eval = in_jets[ii:-1]\n",
    "            njets = np.append(njets, len(in_jets))\n",
    "\n",
    "        ## structure inputs slightly differently and run inference depending on model\n",
    "        # triton model\n",
    "        if use_triton:\n",
    "            X = {}\n",
    "            c = 0\n",
    "            for k in jets_eval.fields:\n",
    "                X[f'{k}__{c}'] = ak.to_numpy(jets_eval[k])\n",
    "                c += 1\n",
    "                \n",
    "            # triton inference\n",
    "            outputs = triton_model(X)\n",
    "                \n",
    "        # local model   \n",
    "        else:\n",
    "            for k in jets_eval.fields:\n",
    "                X = ak.to_numpy(jets_eval[k])\n",
    "                \n",
    "            # local inference\n",
    "            with torch.no_grad():\n",
    "                local_output = local_model(torch.from_numpy(X))\n",
    "\n",
    "        t = np.append(t, time.time()-t_begin)\n",
    "        \n",
    "    print('Total time elapsed = %f sec'%t[-1])\n",
    "\n",
    "    return njets, t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4075b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to minimize noise due to connection with triton server, take avg results of n trials\n",
    "n = 3\n",
    "local_t = None\n",
    "for ii in range(n):\n",
    "    print('------ Trial %i ------'%ii)\n",
    "    if local_t is None:\n",
    "        local_njets, local_t = process_jets(test_inputs_ak, use_triton=False, batch_size=256)\n",
    "    else:\n",
    "        local_njets, local_t_temp = process_jets(test_inputs_ak, use_triton=False, batch_size=256)\n",
    "        local_t += local_t_temp\n",
    "local_t /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8121b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to minimize noise due to connection with triton server, take avg results of n trials\n",
    "n = 5\n",
    "triton_t = None\n",
    "for ii in range(n):\n",
    "    print('------ Trial %i ------'%ii)\n",
    "    if triton_t is None:\n",
    "        triton_njets, triton_t = process_jets(test_inputs_ak, use_triton=True, batch_size=256)\n",
    "    else:\n",
    "        triton_njets, triton_t_temp = process_jets(test_inputs_ak, use_triton=True, batch_size=256)\n",
    "        triton_t += triton_t_temp\n",
    "triton_t /= n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb98f7",
   "metadata": {},
   "source": [
    "## now plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.CMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "# set height ratios for subplots\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])\n",
    "\n",
    "# the first subplot\n",
    "ax0 = plt.subplot(gs[0])\n",
    "# log scale for axis Y of the first subplot\n",
    "ax0.set_yscale(\"log\")\n",
    "line0, = ax0.plot(local_njets, local_t, color='r')\n",
    "line1, = ax0.plot(triton_njets, triton_t, color='b')\n",
    "\n",
    "# the second subplot\n",
    "# shared axis X\n",
    "ax1 = plt.subplot(gs[1], sharex = ax0)\n",
    "line2, = ax1.plot(local_njets, local_t/triton_t, color='black', linestyle='--')\n",
    "plt.setp(ax0.get_xticklabels(), visible=False)\n",
    "# remove last tick label for the second subplot\n",
    "yticks = ax1.yaxis.get_major_ticks()\n",
    "yticks[-1].label1.set_visible(False)\n",
    "\n",
    "# put legend on first subplot\n",
    "ax0.legend((line0, line1), ('local model', 'triton model'), loc='lower right')\n",
    "\n",
    "ax0.set_ylabel('time elapsed (s)')\n",
    "ax1.set_ylabel('$t_{local}/t_{triton}$')\n",
    "ax1.set_xlabel('# inputs processed')\n",
    "\n",
    "# remove vertical gap between subplots\n",
    "plt.subplots_adjust(hspace=.0)\n",
    "plt.rcParams[\"figure.figsize\"] = (7,6)\n",
    "plt.savefig(\"results/timing_results_resnet50.eps\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e3dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
