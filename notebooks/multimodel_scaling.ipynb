{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65608687",
   "metadata": {},
   "source": [
    "# Multi-model scaling benchmark\n",
    "\n",
    "In this notebook, we will study the performance of the Triton server as multiple machine learning models are processing inference requests on the server at the same time. Each model has it's own queue for inference requests, and the triton server default is to load each model on each server instance such that it shares the GPU resources of that instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fadb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the packages we will need\n",
    "import os\n",
    "from distributed import Client, progress\n",
    "from lpcjobqueue import LPCCondorCluster\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.mlbench import process_function\n",
    "import time\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "from utils.promqueries import get_all_queries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4bcbb",
   "metadata": {},
   "source": [
    "This will request a cluster with $n$ workers (or jobs) of CPUs from the Fermilab LPC. If not using the Fermilab computing centers, this will need to be changed for the system you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e375707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(jobs, **kwargs):\n",
    "    \n",
    "    kwargs.setdefault('cores', 1)\n",
    "    kwargs.setdefault('memory', '3GB')\n",
    "    kwargs.setdefault('disk', '2GB')\n",
    "    # by default transfer all utils and models\n",
    "    kwargs.setdefault('transfer_input_files', [f'{os.getenv(\"BASE\")}/utils', f'{os.getenv(\"BASE\")}/models'])\n",
    "    kwargs.setdefault('log_directory', None)\n",
    "    kwargs.setdefault('death_timeout', 180)\n",
    "    kwargs.setdefault('job_extra_directives', {})\n",
    "    kwargs['job_extra_directives'].update(set_default_proxy(kwargs['job_extra_directives']))\n",
    "\n",
    "    cluster = LPCCondorCluster(**kwargs)\n",
    "\n",
    "    # Scaling up the cluster\n",
    "    print(\"Generating job requests...\", end='')\n",
    "    cluster.scale(jobs)\n",
    "    print('initial jobs generated!')\n",
    "    print(\"Waiting for at least one worker...\", end='')\n",
    "    client = Client(cluster)\n",
    "    client.wait_for_workers(1)\n",
    "    print(\"workers(s) online!\")\n",
    "    print(\"Dashboard available at\", client.dashboard_link)\n",
    "    print(\"Waiting for all (%i) workers...\"%jobs, end='')\n",
    "    client.wait_for_workers(jobs)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return cluster, client\n",
    "\n",
    "def set_default_proxy(job_extra_directives):\n",
    "  \"\"\"\n",
    "  Specifying the the grid certificate proxy to be used by the worker nodes. As\n",
    "  the voms-proxy-init typically store certificates the `/tmp` directory, which is\n",
    "  not accessible to the worker nodes. The returned job_extra_directives will\n",
    "  setup the worker nodes to look for the proxy file in the users home directory.\n",
    "  This function will also scan the input proxy file to make sure it exists and is\n",
    "  valid. If the file is not found, an exception is raised with the command to\n",
    "  generate the proxy file in default location.\n",
    "  \"\"\"\n",
    "  proxyfile = ''\n",
    "  if 'x509userproxy' not in job_extra_directives:\n",
    "    proxyfile = '{0}/x509up_u{1}'.format(os.environ['HOME'], os.getuid())\n",
    "    print('Using default proxy file:', proxyfile)\n",
    "  else:\n",
    "    proxyfile = job_extra_directives['x509userproxy']\n",
    "\n",
    "  # Checking if file is a valid file\n",
    "  if not os.path.isfile(proxyfile):\n",
    "    raise Exception(f\"\"\"\n",
    "    The proxy file {proxyfile} doesn't exist! Create the default proxy using the\n",
    "    following command:\n",
    "    > voms-proxy-init --voms cms --valid 192:00 --out ${{HOME}}/x509up_u${{UID}}\n",
    "    \"\"\")\n",
    "\n",
    "  return {'x509userproxy': proxyfile}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0936ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_args = {'log_directory': '/uscmst1b_scratch/lpc1/3DayLifetime/csavard/'}\n",
    "n_workers = 32\n",
    "cluster, client = create_clusters(n_workers, **cluster_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c696f90",
   "metadata": {},
   "source": [
    "We will now run trials with 1, then 2, then 3, and so on, different ML models all running at the same time. In this example, we run different copies of the same model labeled \"pn_demo_bkg_1/2/3/...\". You can rerun this test using difference configurations for of the triton servre to see how this effects things. For example, we suggest testing different GB slices of the triton instances or manually setting the triton server to assign a separate instance for each model and see how this compares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c115636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a couple of configuration to set\n",
    "n_workers_per_bkgmodel = 4\n",
    "n_models = int(np.floor(n_workers/n_workers_per_bkgmodel))\n",
    "\n",
    "worker_hostnames = list(client.scheduler_info()['workers'].keys())\n",
    "output = np.zeros((n_workers,3))\n",
    "datetimes = []\n",
    "\n",
    "# trials for different number of models running at once\n",
    "for ii in range(n_models):\n",
    "    \n",
    "    #seeds, pseudo-events, batchsize, use triton (True/False), model and version\n",
    "    n_files = 50*(ii+1) # run 50 files per model per worker\n",
    "    if ii>2:\n",
    "        n_files = 10*(ii+1)\n",
    "    n_jets = 10000 # run 5000 jets per file\n",
    "    temp_modellist = [\"pn_demo/1\" if x==0 else \"pn_demo_bkg%i/1\" % x for x in range(ii+1)] # change model name to what you are testing\n",
    "    server_list = [\"triton+grpc://triton.fnal.gov:443/\" for x in range(ii+1)]\n",
    "    workargstriton = [range(n_files), [n_jets]*n_files, [1024]*n_files, \n",
    "                      [True]*n_files, temp_modellist*n_files, server_list*n_files]\n",
    "    \n",
    "    # Triton, N bkg models trial\n",
    "    print('Running %i jets among %i files with %i background models...'%(n_jets,n_files,ii))\n",
    "    dt1 = datetime.now()\n",
    "    futurestriton = client.map(process_function, *workargstriton, pure=False, \n",
    "                               workers=worker_hostnames[:n_workers_per_bkgmodel*(ii+1)], retries=2)\n",
    "    progress(futurestriton, notebook=False)\n",
    "    resulttriton = client.gather(futurestriton)\n",
    "    dt2 = datetime.now()\n",
    "    print('Done!')\n",
    "    \n",
    "    datetimes.append((dt1,dt2,ii))\n",
    "    \n",
    "# save the datetimes of each trial to file to look at later\n",
    "with open('datetimes_saved.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s, %s, %s' % x for x in datetimes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c70f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to close all jobs when the trials are done\n",
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e1d20",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here we will provide some code to plot the throughput as a function of models running in the background. We will only look at how background models affect the foreground model we have chosen to study, but foreground model can be swapped as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a18e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datetimes(filename='datetimes_saved.txt'):\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        mylist = [tuple(map(str.strip, i.split(','))) for i in f]\n",
    "    \n",
    "    datetimes = []\n",
    "    for tup in mylist:\n",
    "        datetimes.append((datetime.strptime(tup[0], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                          datetime.strptime(tup[1], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                          int(tup[2])))\n",
    "    return datetimes\n",
    "\n",
    "def get_info(datetimes):\n",
    "    \n",
    "    out = np.zeros((len(datetimes),3))\n",
    "    for (dt1,dt2,m_bkg) in datetimes:\n",
    "        space='{namespace=\"triton\", prometheus_replica=\"prometheus-user-workload-0\", model=\"pn_demo\"}' # change model you want to test\n",
    "        results, queries, unique_model_versions, unique_gpu_instances = get_all_queries([(dt1,dt2)], '30s', space=space)\n",
    "        \n",
    "        # change metrics to study here\n",
    "        data = pd.concat([results['inf_reqs_net'],results['inf_que_time_net'],results['num_instances']],axis=1)\n",
    "        \n",
    "        n_inst = data.iloc[:,2].max()\n",
    "        out[m_bkg,0] = data.iloc[:,0][data.iloc[:,2]==n_inst][3:-1].mean()\n",
    "        out[m_bkg,1] = data.iloc[:,1][data.iloc[:,2]==n_inst][3:-1].mean()\n",
    "        out[m_bkg,2] = n_inst\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the different tests that were used, like a 20 GB and 40 GB instance split for example\n",
    "datetime_20GB = load_datetimes(\"datetimes_test20GBslices.txt\")\n",
    "out_20GB = get_info(datetime_20GB)\n",
    "\n",
    "datetime_40GB = load_datetimes(\"datetimes_test40GBslices.txt\")\n",
    "out_40GB = get_info(datetime_40GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3aa43",
   "metadata": {},
   "source": [
    "Two performance plots are provided:\n",
    "1. Total throughput of all models vs. number of background models - this shows how the full system is affected as more models are run in parallel\n",
    "2. Throughput of the foreground model vs. number of background models - this shows how other models running in the background affect the foreground model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9796781",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(out_20GB)),out_20GB[:,0], label='20 GB slice', color='red')\n",
    "plt.scatter(range(len(out_40GB)),out_40GB[:,0], label='40 GB slice', color='blue')\n",
    "plt.xlabel('Number of background models', fontsize=14)\n",
    "plt.ylabel('$\\sum_{i} model_i$ throughput [$s^{-1}$]', fontsize=14)\n",
    "plt.legend(loc='best')\n",
    "#plt.savefig('results/sum_throughput_vs_bkgmodel_1instance.eps',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c95cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(out_20GB)),out_20GB[:,0], label='20 GB slice', color='red')\n",
    "#plt.plot(np.linspace(0,3,20), out_20GB[0,0]/(np.linspace(0,3,20)+1), color='red', linestyle='dashed')\n",
    "plt.scatter(range(len(out_40GB)),out_40GB[:,0], label='40 GB slice', color='blue')\n",
    "#plt.plot(np.linspace(0,7,40), out_40GB[0,0]/(np.linspace(0,7,40)+1), color='blue', linestyle='dashed')\n",
    "plt.plot([],[], color='black', linestyle='dashed', label='Perfect slice sharing')\n",
    "plt.xlabel('Number of background models', fontsize=14)\n",
    "plt.ylabel('Demo model throughput [$s^{-1}$]', fontsize=14)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b09508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea for triton",
   "language": "python",
   "name": "coffea-triton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
